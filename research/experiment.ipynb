{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Medical_chatbot\\venv\\lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents extracted: 77\n"
     ]
    }
   ],
   "source": [
    "# Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"D:/Medical_chatbot/data/\")\n",
    "print(\"Documents extracted:\", len(extracted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of my chunks: 233\n"
     ]
    }
   ],
   "source": [
    "# Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of my chunks:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "api_key = \"b0b29cdd-c3d5-40a9-8c81-7f75f4a19bf3\"\n",
    "pc = pinecone.Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"medical\"\n",
    "\n",
    "# Create the index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 233/233 [00:51<00:00,  4.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Generate embeddings for text chunks and prepare for upsert\n",
    "def generate_embeddings(text_chunks, embeddings):\n",
    "    embedded_texts = []\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Generating embeddings\")):\n",
    "        vector = embeddings.embed_query(chunk.page_content)\n",
    "        embedded_texts.append({\n",
    "            \"id\": f\"chunk_{i}\",\n",
    "            \"values\": vector,\n",
    "            \"metadata\": {\"text\": chunk.page_content}\n",
    "        })\n",
    "    return embedded_texts\n",
    "\n",
    "embedded_texts = generate_embeddings(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserting embeddings: 100%|██████████| 2/2 [00:07<00:00,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted text chunks into Pinecone index successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Upsert embeddings to Pinecone index\n",
    "batch_size = 128  # Define your batch size\n",
    "\n",
    "for i in tqdm(range(0, len(embedded_texts), batch_size), desc=\"Upserting embeddings\"):\n",
    "    batch = embedded_texts[i:i + batch_size]\n",
    "    to_upsert = list(zip([item[\"id\"] for item in batch], \n",
    "                         [item[\"values\"] for item in batch], \n",
    "                         [item[\"metadata\"] for item in batch]))\n",
    "    index.upsert(vectors=to_upsert)\n",
    "\n",
    "print(\"Upserted text chunks into Pinecone index successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def query_pinecone(query: str, top_k: int, index, embeddings):\n",
    "\n",
    "#     # Generate the embedding for the question\n",
    "#     query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "#     # Perform the similarity search\n",
    "#     query_results = index.query(vector=query_vector, top_k=top_k, include_values=True, include_metadata=True)\n",
    "    \n",
    "#     # Extract and print the text content of the top matches\n",
    "#     for match in query_results['matches']:\n",
    "#         print(f\"Score: {match['score']}\")\n",
    "#         print(f\"Text Content: {match['metadata'].get('text', 'Text not found')}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# query = \"What is COMMON LLM ELIGIBILITY CRITERIA\"\n",
    "# docs=query_pinecone(query, top_k=3, index=index, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import getpass\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# Initialize Groq client\n",
    "import getpass\n",
    "groq_api_key = getpass.getpass(\"Enter your Groq API key: \")\n",
    "groq_client = Groq(api_key=groq_api_key)\n",
    "\n",
    "\n",
    "def query_pinecone(query: str, top_k: int, index, embeddings):\n",
    "    # Generate the embedding for the question\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    \n",
    "    # Perform the similarity search\n",
    "    query_results = index.query(vector=query_vector, top_k=top_k, include_values=True, include_metadata=True)\n",
    "    \n",
    "    # Extract and print the text content of the top matches\n",
    "    docs = []\n",
    "    for match in query_results['matches']:\n",
    "        print(f\"Score: {match['score']}\")\n",
    "        text_content = match['metadata'].get('text', 'Text not found')\n",
    "        print(f\"Text Content: {text_content}\\n\")\n",
    "        docs.append(text_content)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(query: str, docs: list[str]):\n",
    "    # Construct the system message\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant that answers questions about AI using the context provided below.\\n\\n\"\n",
    "        \"CONTEXT:\\n\" + \"\\n\\n---\\n\".join(docs) + \"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    # Prepare the messages for the chat\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "    \n",
    "    # Generate and return the response\n",
    "    chat_response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return chat_response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.796412706\n",
      "Text Content: COMMON LLM ELIGIBILITY CRITERIA  \n",
      "Candidates are eligible to apply for admission in an LLM course if they have completed their \n",
      "graduation in law, that is, if the aspirant has secured his/ her LLB/ Bachelor of Law/ Five -\n",
      "year integrated LLB course from a recognised university/ college.  \n",
      "Apart from this, some colleges also fix a minimum percentage requirement for the LLM \n",
      "course offered by them. As part of the eligibility criteria shared by such colleges, candidates\n",
      "\n",
      "Score: 0.616188467\n",
      "Text Content: LLM EXAMINATION  \n",
      "LLM is a post -graduate degree of law of two -year course that is pursued after completion of \n",
      "law degree to become knowledgeable in any specialisation. Admission is done on basis of \n",
      "performance in law entrance exams like CLAT, LSAT, and other university - level \n",
      "examinations. A law degree of three or five years with 50 -60% total scores are required.  \n",
      "LLM admissions are taken through examinations. Plenty of private law schools provide LLM\n",
      "\n",
      "According to the context, the COMMON LLM ELIGIBILITY CRITERIA are:\n",
      "\n",
      "* Completion of graduation in law, i.e., securing LLB/ Bachelor of Law/ Five-year integrated LLB course from a recognized university/ college.\n",
      "* Some colleges may also have a minimum percentage requirement for the LLM course offered by them.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "query = \"What is COMMON LLM ELIGIBILITY CRITERIA\"\n",
    "docs = query_pinecone(query, top_k=2, index=index, embeddings=embeddings)\n",
    "\n",
    "out = generate(query=query, docs=docs)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
