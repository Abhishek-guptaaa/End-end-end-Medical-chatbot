{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Extract data from the PDF\n",
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "extracted_data = load_pdf(\"D:/Medical_chatbot/data/\")\n",
    "print(\"Documents extracted:\", len(extracted_data))\n",
    "\n",
    "# Create text chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = text_split(extracted_data)\n",
    "print(\"Length of my chunks:\", len(text_chunks))\n",
    "\n",
    "# Download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()\n",
    "\n",
    "# Initialize Pinecone\n",
    "api_key = \"b0b29cdd-c3d5-40a9-8c81-7f75f4a19bf3\"\n",
    "pc = pinecone.Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"medical\"\n",
    "\n",
    "# Create the index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=pinecone.ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Generate embeddings for text chunks and prepare for upsert\n",
    "def generate_embeddings(text_chunks, embeddings):\n",
    "    embedded_texts = []\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"Generating embeddings\")):\n",
    "        vector = embeddings.embed_query(chunk.page_content)\n",
    "        embedded_texts.append({\n",
    "            \"id\": f\"chunk_{i}\",\n",
    "            \"values\": vector,\n",
    "            \"metadata\": {\"text\": chunk.page_content}\n",
    "        })\n",
    "    return embedded_texts\n",
    "\n",
    "embedded_texts = generate_embeddings(text_chunks, embeddings)\n",
    "\n",
    "# Upsert embeddings to Pinecone index\n",
    "for i in tqdm(range(0, len(embedded_texts), 100), desc=\"Upserting embeddings\"):\n",
    "    batch = embedded_texts[i:i+100]\n",
    "    index.upsert(vectors=batch)\n",
    "\n",
    "print(\"Upserted text chunks into Pinecone index successfully.\")\n",
    "\n",
    "# Query the Pinecone index\n",
    "query_results = index.query(\n",
    "    vector=embedded_texts[0]['values'],\n",
    "    top_k=3,\n",
    "    include_values=True\n",
    ")\n",
    "\n",
    "print(\"Query results:\", query_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your question\n",
    "question = \"What causes asthma?\"\n",
    "\n",
    "# Generate the embedding for the question\n",
    "question_embedding = embeddings.embed_query(question)\n",
    "\n",
    "# Perform the similarity search using the generated question embedding\n",
    "query_results = index.query(vector=question_embedding,top_k=3,include_values=True)\n",
    "\n",
    "# Create a dictionary to map chunk IDs to their text content\n",
    "id_to_text = {doc['id']: doc['metadata']['text'] for doc in embedded_texts}\n",
    "\n",
    "# Extract and print the text content of the top matches\n",
    "for match in query_results['matches']:\n",
    "    chunk_id = match['id']\n",
    "    score = match['score']\n",
    "    text_content = id_to_text.get(chunk_id, \"Text not found\")\n",
    "    print(f\"Chunk ID: {chunk_id}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Text Content: {text_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "load_dotenv()\n",
    "groq_api_key=os.getenv('GROQ_API_KEY')\n",
    "\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key, model_name=\"Gemma-7b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
